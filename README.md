# ğŸ§  AI Chat Cleaner  
### Offline LLM-Based Conversational Parser â€” Built with Mistral 7B + llama.cpp  

![Python](https://img.shields.io/badge/python-3.11+-blue?logo=python)
![LLM](https://img.shields.io/badge/Model-Mistral_7B_Instruct-orange)
![License](https://img.shields.io/badge/license-MIT-green)
![Offline](https://img.shields.io/badge/Mode-Offline_CPU-yellow)
![Status](https://img.shields.io/badge/Regex-Free-success)

---

## ğŸš€ Overview

The **AI Chat Cleaner** transforms messy multiformat chat transcripts (Excel / PDF / text) into a clean structured dataset using **local LLM reasoning**.  
No fragile regexes â€” just true language understanding.

It uses a quantized **Mistral-7B-Instruct** model via [`llama.cpp`](https://github.com/ggerganov/llama.cpp), runs entirely on CPU, and outputs perfectly formatted CSV/JSON with:
- `time`
- `speaker`
- `role`
- `message`

---

## ğŸ§© Key Features

âœ… Works fully **offline** (no API keys)  
âœ… Handles **Excel, CSV, TXT, and PDF** inputs  
âœ… **No regex** â€” pure AI structural parsing  
âœ… Automatically splits multi-speaker lines (e.g. `ok. since when? neha- today only`)  
âœ… **JSON-grammar enforcement** ensures valid structure  
âœ… Auto-repair for malformed JSON  
âœ… **Chunked + cached processing** for large transcripts  
âœ… CPU-friendly quantized GGUF model  

---

## âš™ï¸ Tech Stack

| Layer | Technology | Why |
|:------|:------------|:----|
| Language | **Python 3.11+** | Fast prototyping, clean data pipelines |
| Model Runtime | **llama-cpp-python** | Offline inference for `.gguf` models |
| Model | **Mistral-7B-Instruct Q4 K M (Q4 _0)** | Compact yet accurate dialogue understanding |
| Data Frames | **pandas** | Excel / CSV IO |
| PDF Parsing | **pypdf** | Extract chat text from pages |
| Parallelism | **ThreadPoolExecutor** | Multi-file / multi-row concurrency |
| Prompting | **SYSTEM + USER templates** | Deterministic structured outputs |
| Validation | **JSON Grammar + Auto Repair** | Guarantees valid output |
| Caching | **MD5 hash-based local cache** | Skip already-processed chunks |

---

## ğŸ—ï¸ Architecture

```
chat-cleaner-ollama/
â”‚
â”œâ”€â”€ run_all_samples.py            # orchestrator
â”œâ”€â”€ models/                       # local GGUF model
â”œâ”€â”€ out/                          # generated CSV outputs
â”œâ”€â”€ samples/                      # raw test chats
â”‚
â””â”€â”€ src/chat_cleaner/
    â”œâ”€â”€ pipeline.py               # core workflow
    â”œâ”€â”€ io_utils.py               # input discovery (Excel/CSV/TXT/PDF)
    â”œâ”€â”€ preclean.py               # light cleanup + speaker split
    â”œâ”€â”€ ai_normalizer.py          # safe formatting for LLM
    â”œâ”€â”€ llamacpp_client.py        # llama.cpp integration
    â”œâ”€â”€ prompts.py                # system / user prompt templates
    â”œâ”€â”€ segmentation.py           # optional AI chunk segmentation
    â””â”€â”€ cache/                    # auto-generated JSON cache
```

---

## ğŸ§± Data Flow

### 1ï¸âƒ£ Input Discovery  
`io_utils.py` scans supported files â†’ yields `(conversation_id, raw_text)` pairs.  
Each Excel row = one conversation; each PDF = aggregated text.

### 2ï¸âƒ£ Pre-cleaning & Normalization  
`preclean_text()` removes boilerplate and injects line breaks between possible speakers.  
`normalize_for_llm()` standardizes spacing, tabs, and chunk sizes.

### 3ï¸âƒ£ Chunking + Caching  
Long conversations are split into overlapping 1200â€“1500 char chunks.  
Each chunkâ€™s MD5 hash is cached â†’ instant re-runs.

### 4ï¸âƒ£ Model Inference  
`llamacpp_client.py` loads the quantized Mistral model:

```python
llm = Llama(
    model_path="models/mistral-7b-instruct-v0.2.Q4_K_M.Q4_0.gguf",
    n_ctx=32768,
    n_threads=8,
    n_gpu_layers=0
)
```

Grammar mode constrains the model to emit a valid JSON array.

### 5ï¸âƒ£ Prompt Design  
- **System Prompt:** Defines keys, roles, and multi-speaker split logic.  
- **User Prompt:** Injects actual chat text under strict format instructions.

### 6ï¸âƒ£ Post-Processing  
Outputs are validated via `json.loads()` and auto-repaired if needed (`try_repair_json`).  
Each conversation â†’ a dedicated CSV in `/out`.

---

## ğŸ“Š Example

### Input (Text / Excel Cell)
```
Ravi : ok. since when? neha- today only.
ravi: press f8 whn restart. tell me what happen
user: safe mode opened. (yay)
```

### Output (CSV)
| time | speaker | role | message |
|------|----------|------|----------|
| null | Ravi | Agent | ok. since when? |
| null | Neha | User | today only. |
| null | Ravi | Agent | press f8 whn restart. tell me what happen |
| null | User | User | safe mode opened. (yay) |

---

## âš¡ Why LLM > Regex

| Regex Parser | AI Parser |
|:--------------|:-----------|
| Hard-coded patterns per format | Learns contextually from text |
| Breaks on typos / emojis | Robust to natural dialogue |
| Needs manual tuning | Generalizes automatically |
| Limited to known syntax | Understands unseen chat formats |

---

## ğŸ§  Model Details

| Setting | Value |
|:---------|:-------|
| Model File | `mistral-7b-instruct-v0.2.Q4_K_M.Q4_0.gguf` |
| Runtime | `llama-cpp-python` |
| Context Window | 32 k tokens |
| Threads | 8 (default CPU cores) |
| Temperature | 0.0 (deterministic) |
| Output Grammar | JSON array of objects |
| Cache | JSON per chunk (MD5) |

---

## âš™ï¸ Installation

```bash
# 1ï¸âƒ£ Clone the project
git clone https://github.com/<yourname>/chat-cleaner-ollama.git
cd chat-cleaner-ollama

# 2ï¸âƒ£ Create venv
python -m venv .venv
.\.venv\Scriptsctivate  # Windows

# 3ï¸âƒ£ Install deps
pip install -r requirements.txt

# 4ï¸âƒ£ Place model
models/mistral-7b-instruct-v0.2.Q4_K_M.Q4_0.gguf

# 5ï¸âƒ£ Run cleaner
python run_all_samples.py --model "models/mistral-7b-instruct-v0.2.Q4_K_M.Q4_0.gguf"
```

---

## ğŸ“¦ Example Run

```bash
ğŸ§  Found 2 sample files:
   - chat_sample_xl.xlsx
   - chat_sample.pdf

ğŸš€ Processing samples/chat_sample_xl.xlsx
âœ… Processed: chat_sample_xl_row1 â†’ chat_sample_xl_row1_clean.csv
âœ… Processed: chat_sample_xl_row2 â†’ chat_sample_xl_row2_clean.csv

ğŸ¯ All chats processed successfully!
```

Outputs appear in the `out/` folder as individual CSVs.

---

## ğŸ“ˆ Performance Notes

| Metric | Typical |
|:--------|:--------|
| CPU RAM | â‰ˆ 8â€“10 GB |
| Avg Speed | 20â€“30 s per chunk |
| Context Window | 32 k tokens |
| Accuracy | â‰ˆ 90 % speaker segmentation on noisy text |
| Retry Mechanism | Automatic JSON repair & re-inference |

---

## ğŸ› ï¸ Future Roadmap

| Feature | Description |
|:----------|:-------------|
| ğŸ§© Async batch inference | Parallel per-file streaming |
| ğŸ§  Auto segmentation | Detect session start / end boundaries |
| ğŸ’¬ Dialogue linking | Merge consecutive messages per user |
| â˜ï¸ Web interface | Streamlit/Flask upload UI |
| ğŸ” Entity Extraction | Tickets, emails, numbers |
| ğŸ§  Pluggable models | Swap in Llama 3 / Mixtral easily |

---


